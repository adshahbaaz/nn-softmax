{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction**\n",
        "\n",
        "In this notebook, we implement a neural net with a softmax classifier trained on the iris dataset. The code highlights important calculations in the derivation of the backprop equations through inline comments. We achieve a 99% accuracy after training for $10000$ epochs.\n"
      ],
      "metadata": {
        "id": "W6totYIIvm24"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qe6NtgPQ5It"
      },
      "source": [
        "## **Load iris data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "1xaIdtVRhhSq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wl5aPS25gv62",
        "outputId": "e3baec53-d67a-4394-c8d2-1e7aca493168"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-25 15:14:10--  https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘iris.data.1’\n",
            "\n",
            "iris.data.1             [ <=>                ]   4.44K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-04-25 15:14:10 (63.8 MB/s) - ‘iris.data.1’ saved [4551]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "HUAzRzSzhoUr"
      },
      "outputs": [],
      "source": [
        "iris = pd.read_csv(\"iris.data\",delimiter= \",\",header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "vu3mNFbnG2Es"
      },
      "outputs": [],
      "source": [
        "# create train and test split\n",
        "\n",
        "n_train = 110\n",
        "n_test = 40\n",
        "\n",
        "iris_train = iris.sample(n_train)\n",
        "iris_test = iris.sample(n_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTS8bJDSj9AN",
        "outputId": "942c8ba1-af4d-425d-b839-5e9f18064494"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       0    1    2    3                4\n",
            "69   5.6  2.5  3.9  1.1  Iris-versicolor\n",
            "107  7.3  2.9  6.3  1.8   Iris-virginica\n",
            "17   5.1  3.5  1.4  0.3      Iris-setosa\n",
            "9    4.9  3.1  1.5  0.1      Iris-setosa\n",
            "127  6.1  3.0  4.9  1.8   Iris-virginica\n",
            "..   ...  ...  ...  ...              ...\n",
            "77   6.7  3.0  5.0  1.7  Iris-versicolor\n",
            "46   5.1  3.8  1.6  0.2      Iris-setosa\n",
            "100  6.3  3.3  6.0  2.5   Iris-virginica\n",
            "115  6.4  3.2  5.3  2.3   Iris-virginica\n",
            "140  6.7  3.1  5.6  2.4   Iris-virginica\n",
            "\n",
            "[110 rows x 5 columns]\n"
          ]
        }
      ],
      "source": [
        "print(iris_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Pin-CVu6ipz0"
      },
      "outputs": [],
      "source": [
        "features = iris_train.iloc[:,0:4]\n",
        "targets = iris_train.iloc[:,4].astype(\"category\").cat.codes\n",
        "labels = iris_train.iloc[:,4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLhEfWKemgnx",
        "outputId": "4952dc75-9781-42b3-fdc7-b021d25555e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "69     Iris-versicolor\n",
            "107     Iris-virginica\n",
            "17         Iris-setosa\n",
            "9          Iris-setosa\n",
            "127     Iris-virginica\n",
            "            ...       \n",
            "77     Iris-versicolor\n",
            "46         Iris-setosa\n",
            "100     Iris-virginica\n",
            "115     Iris-virginica\n",
            "140     Iris-virginica\n",
            "Name: 4, Length: 110, dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![The neural network we implement](./nn-backprop.jpg)"
      ],
      "metadata": {
        "id": "BpBcKJsTeqxc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HGYYvj7r2gJ"
      },
      "source": [
        "## **Define activation and loss functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "QE-2XQ23ohUH"
      },
      "outputs": [],
      "source": [
        "def relu(x):\n",
        "  return np.maximum(0,x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "7tYOr5EiK3Fk"
      },
      "outputs": [],
      "source": [
        "def softmax(x):\n",
        "    # For numerical stability, subtract the maximum value along the axis\n",
        "    # This prevents potential overflow issues with large exponentials.\n",
        "    x_max = np.max(x, axis=-1, keepdims=True)  # Keepdims, essential for broadcasting, axis will be along columns for 2d arrays since -1 selects last dimension\n",
        "    shifted_x = x - x_max\n",
        "    exp_x = np.exp(shifted_x)\n",
        "    # Calculate the sum of exponentials along the specified axis\n",
        "    sum_exp_x = np.sum(exp_x, axis=-1, keepdims=True)\n",
        "    softmax_output = exp_x / sum_exp_x\n",
        "    return softmax_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "_hDsGUe5SCCJ"
      },
      "outputs": [],
      "source": [
        "def cross_entropy_loss(y, y_pred):\n",
        "    # Clipping for numerical stability (prevents log(0))\n",
        "    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)  # Clip to avoid log(0) or log(1)\n",
        "\n",
        "    # Calculate cross-entropy loss\n",
        "    loss = -np.mean(y * np.log(y_pred)) # Element-wise multiplication then mean\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqObhCxF9Vja"
      },
      "source": [
        "## **Initialise the layers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "collapsed": true,
        "id": "SBLYzByeWdSj"
      },
      "outputs": [],
      "source": [
        "X = np.array(features.values) # Input layer\n",
        "y = np.array(targets.values,dtype = \"i\") # targets\n",
        "num_classes = 3 # Number of classes from predictions\n",
        "y = np.eye(num_classes)[y] # one-hot encoded for categories\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "XgLuIwnDpNCw"
      },
      "outputs": [],
      "source": [
        "# mean standardise features\n",
        "mu = X.mean(axis = 0)\n",
        "sd = X.std(axis = 0)\n",
        "X = (X - mu) / sd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "_ddfpZy869SV"
      },
      "outputs": [],
      "source": [
        "#dimensions of our neural net\n",
        "'''\n",
        "Sizes:\n",
        " - m : number of examples in the dataset\n",
        " - nx : input size (number of variables being considered)\n",
        " - ny : output size (or number of classes)\n",
        " - nh: number of hidden units of the lth layer\n",
        " - L : number of layers in the network.\n",
        "'''\n",
        "\n",
        "m = X.shape[0]\n",
        "nx = X.shape[1]\n",
        "ny = np.unique(targets.values).shape[0]\n",
        "nh = [nx,8] # 8 hidden neurons is overkill, can reduce to 5 for good performance\n",
        "L = len(nh) + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTc7gjFMLRyO"
      },
      "source": [
        "## **Neural network (Softmax activation)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "u6I8N5hc8MMU"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "We build a class to keep the code cleaner and reusable.\n",
        "The code highlights the simplicity and elegance of the mathematics.\n",
        "Because of how matrix multiplication works the inputs to each layer will always\n",
        "have a number of rows equal to |X| = m, where m is the number of observations\n",
        "in our batch.\n",
        "It is also an interesting point that we find it hard to visualise anything\n",
        "greater than 3 dimensions but can easily solve equations in\n",
        "> 768 dimensional space with linear algebra and A-level calculus!\n",
        "'''\n",
        "\n",
        "class NN_softmax:\n",
        "  def __init__(self):\n",
        "\n",
        "    # Initialise weights and biases\n",
        "    self.W = np.random.rand(nh[1],nx)\n",
        "    self.WL = np.random.rand(ny,nh[1])\n",
        "    '''\n",
        "    each row represents the weights for a neuron in the next layer nh[l+1]\n",
        "    and the columns the number of neurons in the current layer (nh[l] = nx when l = 0)\n",
        "    '''\n",
        "    self.b = np.zeros([1,nh[1]])\n",
        "    self.bL = np.zeros([1,ny])\n",
        "\n",
        "    '''\n",
        "    each column represents the bias for a neuron in the layer nh[l+1]\n",
        "    initialised as row vector of 0s. (only hidden and output layers use biases).\n",
        "    '''\n",
        "\n",
        "    '''\n",
        "    Next we, tack on weights with biases.\n",
        "    We will append column vector of 1s to inputs so that weighted sum can\n",
        "    be expressed as single dot product\n",
        "    '''\n",
        "    self.W = np.concatenate((self.W,self.b.T),axis = 1) #along columns\n",
        "    self.WL = np.concatenate((self.WL,self.bL.T),axis = 1)\n",
        "\n",
        "\n",
        "  def forward(self,X,y):\n",
        "    m = len(y) # number of observations\n",
        "    self.X = np.concatenate((X,np.ones([m,1])),axis = 1) # append column of 1s to X\n",
        "    self.y = y\n",
        "    W = self.W\n",
        "    WL = self.WL\n",
        "    self.z = self.X @ W.T # + b is implicit\n",
        "    self.a = np.concatenate((relu(self.z),np.ones([m,1])),axis = 1) # append column of 1s\n",
        "    zL =  self.a @ WL.T # + bL is implicit\n",
        "    self.aL = softmax(zL)\n",
        "    C = cross_entropy_loss(y,self.aL)\n",
        "\n",
        "    return C\n",
        "\n",
        "  def backprop(self):\n",
        "    X = self.X\n",
        "    y = self.y\n",
        "    aL =  self.aL\n",
        "    a = self.a\n",
        "    z = self.z\n",
        "    WL = self.WL\n",
        "\n",
        "    # /* output layer gradients */\n",
        "\n",
        "    dC_dzL = aL - y # we can use this since y is one-hot encoded\n",
        "    '''\n",
        "    here matrix multiplication sums the partials across each\n",
        "    observation for a total contribution to the loss\n",
        "    NOT to apply the total-derivative which would be incorrect.\n",
        "    '''\n",
        "    #vector chain rule says to multiply the partials dC/dzL @ dzL/dwL\n",
        "    self.dC_dwL =  (aL - y).T @ a #(150,3) @ (150,8)\n",
        "\n",
        "   # /* hidden layer gradients */\n",
        "\n",
        "    dzL_da = WL[:,:-1] #crucial how hidden layers output influences zL\n",
        "                       #exclude the last column as these are biases\n",
        "    dz_dw = X\n",
        "    #we use relu activation derivtaive is a matrix of 1s and 0s.\n",
        "    da_dz = (z > 0).astype(int)\n",
        "    '''\n",
        "    matrix multiply introduces total-derivative here!\n",
        "    '''\n",
        "    dC_dz = (dC_dzL @ dzL_da) * da_dz #dim of matrices: (150,3) @ (3,8) * (150,8)\n",
        "\n",
        "    '''\n",
        "    finally we sum partails across observations using matmul as before\n",
        "    and NOT to introduce the total-derivative.\n",
        "    '''\n",
        "    self.dC_dw =  dC_dz.T @ dz_dw\n",
        "    return\n",
        "\n",
        "  def optimise(self):\n",
        "      learning_rate = 0.01\n",
        "      self.WL = self.WL - (learning_rate * self.dC_dwL)\n",
        "      self.W = self.W - (learning_rate * self.dC_dw)\n",
        "      #we dont need to update biases seperately as WL and W include them\n",
        "      return\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k16LDDiE9nsg"
      },
      "source": [
        "## **Training and Evaluation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdzWMv24SREw"
      },
      "source": [
        "### **Train**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "jrrtaTp8lpCK"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42) # set seed for reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "BK3jJ3LyKeHL"
      },
      "outputs": [],
      "source": [
        "nn = NN_softmax()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qUk84MRKjMg",
        "outputId": "9c15ae90-62dc-4693-9c15-e1297e088252"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epochs: 100%|██████████| 10000/10000 [00:13<00:00, 758.48it/s, Cost: 0.0002729270083395493]\n"
          ]
        }
      ],
      "source": [
        "# batch gradient descent (entire dataset at once as opposed to mini batches or stochastic gradient descent (mini_batchsize = 1))\n",
        "import time\n",
        "import tqdm\n",
        "\n",
        "epochs  = 10000\n",
        "cost = 0\n",
        "pbar = tqdm.tqdm(range(epochs),desc  =\"Epochs\")\n",
        "\n",
        "for e in pbar:\n",
        "    #time.sleep(.05)\n",
        "    cost = nn.forward(X,y)\n",
        "    pbar.set_postfix_str(f\"Cost: {cost}\")\n",
        "    nn.backprop()\n",
        "    nn.optimise()\n",
        "    #nn.aL # predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LgKh6vWmyjR"
      },
      "source": [
        "### **Evaluate**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "RUwd_9J_QVfo"
      },
      "outputs": [],
      "source": [
        "features_test = iris_test.iloc[:,0:4]\n",
        "targets_test = iris_test.iloc[:,4].astype(\"category\").cat.codes\n",
        "labels_test = iris_train.iloc[:,4]\n",
        "\n",
        "m_test = len(iris_test)\n",
        "\n",
        "X_test = np.array(features_test.values) # Input layer\n",
        "y_test = np.array(targets_test.values,dtype = \"i\") # targets\n",
        "labels_test = np.array(labels_test.values) # map\n",
        "#print(X)\n",
        "\n",
        "# mean standardise features\n",
        "mu = X_test.mean(axis = 0)\n",
        "sd = X_test.std(axis = 0)\n",
        "X_test = (X_test - mu) / sd\n",
        "\n",
        "num_classes = 3 # Number of classes from predictions\n",
        "y_test = np.eye(num_classes)[y_test] # one-hot encoded for categories\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtrfSVuon7Jz",
        "outputId": "3adb50aa-b86e-4bae-ecd5-626e1a4173ae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.5420465902046937)"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ],
      "source": [
        "nn.forward(X_test,y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnhQm28WKPs2"
      },
      "source": [
        "### **Metrics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dy7OzQKxKN71",
        "outputId": "1ea94cee-a25b-459b-aaa0-73654a008b51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test: 0.35\n"
          ]
        }
      ],
      "source": [
        "preds = nn.aL\n",
        "preds_onehot = np.zeros_like(preds)\n",
        "preds_onehot[range(len(preds)),preds.argmax(axis = 1)] = 1 # set predicted category to 1\n",
        "true_positives =  np.all(preds_onehot == y_test,axis = 1)\n",
        "\n",
        "accuracy = np.sum(true_positives) / len(preds)\n",
        "\n",
        "print(\"Accuracy on test:\",accuracy)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}